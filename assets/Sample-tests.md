# Sample tests

Goal of our framework is to enable and automate testing for LLM-based agents. The framework adapts the classical software development testing pyramid to the domain of LLM-based agents, ranging from unit tests to end-to-end testing. It facilitates structural testing of individual agent components, their internal and external interactions, and acceptance testing, ultimately evaluating the agent from an end-user perspective. 

Designed to support testing of agents, their components, and interactions under various conditions with definable inputs, the framework assists developers in detecting and monitoring different error scenarios. Given that LLM-based agents are prone to systematic quality issues such as hallucinations, our framework helps with making these occurrences transparent to developers. 

This file contains some exemplary tests on level unit, integration and acceptance tests. 

## Unit tests

| Test focus           | Test Description                                             | Rationale                                                                    | Example                                                                                                          |
|-------------------------------|------------------------------------------------------------------------|----------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------|
| Agent               | Agent initialization with valid configuration                          | Ensure agent can be created with required parameters (model\_id, system\_prompt, etc.) | Test if _BedrockConverseAgent (model\_id="claude-3-sonnet")_ is created successfully                                         |
| Agent               | Agent initialization with invalid model ID fails                       | Test input validation and error handling                                               | Test that invalid model IDs raise appropriate exceptions                                                                   |
| Agent               | Agent conversation ID generation and uniqueness                        | Each new conversation gets a unique, valid ID for session management                   | Verify _agent.conversation_id_ is unique across multiple agent instances                                           |
| Agent               | Agent reset functionality clears conversation state                    | Reset should clear conversation history and generate new conversation ID               | Test _agent.reset()_ clears agent.messages and changes conversation\_id                                             |
| Conversation history| In-memory conversation history stores and retrieves messages correctly | Basic CRUD operations work for conversation persistence                                | Test adding user/ assistant messages and retrieving full conversation                                                      |
| Conversation history| Conversation history table exists and accessible                       | Infrastructure dependency available and configured correctly                           | Check table exists, has correct schema (primary/ secondary key), and agent can read/ write table                           |
| Conversation history| Conversation history pagination for long conversations                 | Large conversations don't cause memory issues or API limits                            | Test retrieving conversations with 100+ messages                                                                           |
| Conversation history| Conversation history isolation by auth context                         | Users can only access their own conversations for security                             | Test that conversations with different auth contexts are isolated                                                          |
| Conversation history| Conversation history message ordering and timestamps                   | Messages are stored and retrieved in correct chronological order                       | Verify message sequence is preserved across save/load cycles                                                               |
| Knowledge base/ RAG | Document embedding and storage successful                              | Documents can be processed and stored in vector database                               | Test upload of PDF/ text file and check embeddings are created                                                             |
| Knowledge base/ RAG | Semantic search returns relevant documents                             | Search queries return contextually relevant content                                    | Test query "weather in Paris" returns weather-related documents, not tourism documents                                     |
| Knowledge base/ RAG | Knowledge base handles empty/ malformed queries gracefully             | System doesn't crash on edge cases                                                     | Test empty strings, very long queries, special characters                                                                  |
| Knowledge base/ RAG | Document chunking preserves context and meaning                        | Text splitting doesn't break sentences or lose important context                       | Verify chunks maintain semantic coherence have proper overlap |
| Knowledge base/ RAG | Vector similarity search performance within acceptable limits          | Query response time is under threshold (e.g., 200ms)                                   | Measure and assert query latency for knowledge base searches                                                               |
| LLM                 | Bedrock Converse API connectivity and authentication                   | Agent can successfully connect to and authenticate with Bedrock                        | Test basic LLM call succeeds with valid AWS credentials                                                                    |
| LLM                 | LLM response parsing and error handling                                | Responses from LLM correctly parsed into expected format                               | Test handling of streaming responses, tool calls, reasoning output                                                         |
| LLM                 | Model parameter validation (temperature, max\_tokens, etc.)            | Invalid model parameters are caught before API calls                                   | Test temperature > 1.0 or negative values are rejected                                                                     |
| LLM                 | Guardrails integration and content filtering                           | Bedrock Guardrails properly filter inappropriate content                               | Test blocked content returns appropriate error, not raw LLM response                                                       |
| Prompt              | System prompt injection and formatting                                 | System prompts properly formatted and included in LLM requests                         | Verify system prompt appears correctly in conversation context                                                             |
| Prompt              | User input sanitization and validation                                 | Malicious or malformed inputs are handled safely                                       | Test injection attempts, extremely long inputs, binary data                                                                |
| Prompt              | Variable substitution in prompt template                               | Dynamic prompts with variables render correctly                                        | Test if variable _user_name_ gets properly substituted in prompt                                                   |
| Prompt              | Context window management and truncation                               | Long conversations truncated appropriately to fit model limits                         | Test conversation with 200K tokens gets truncated intelligently                                                            |
| Tools               | Tool registration with valid Python functions                          | Functions with proper type hints and docstrings register successfully                  | Test if _agent.register_tool(get_weather)_ works correctly                                                       |
| Tools               | Tool registration validation catches invalid tools                     | Prevent tool registration without proper signatures or documentation                   | Test registering function without doc-string or type hints fails appropriately (negative test)                             |
| Tools               | Individual tool execution with valid inputs                            | Each registered tool can be called directly and returns expected output                | Test tool _get_weather("London")_ returns structured weather data for London                                      |
| Tools               | Tool error handling for invalid inputs                                 | Tools gracefully handle missing or malformed parameters                                | Test tool behavior with None, wrong types, or missing required parameters                                                  |
| Tools               | Tool timeout handling for long-running operations                      | Tools that take too long are terminated to prevent hanging                             | Test that tool sleeping for e.g. 30s times out appropriately                                                               |
| Tracing             | Trace collection initialization and configuration                      | Tracing system starts up correctly with chosen tracer                                  | Test InMemoryTracer, DynamoDbTracer initialization (Generative AI toolkit infrastructure functions)                        |
| Tracing             | Trace data completeness and accuracy                                   | All expected trace attributes are captured correctly                                   | Verify trace contains ai.user.input, ai.agent.response, timestamps, etc.  (Generative AI toolkit infrastructure functions) |
| Tracing             | Trace storage and retrieval functionality                              | Traces can be stored and retrieved without data loss                                   | Test storing traces to DynamoDB and retrieving them completely                                                             |


## Integration tests

| Test focus                     | Test Description                                  | Rationale                                                          | Example                                                                                         |
|------------------------------------------|-------------------------------------------------------------|------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------|
| Agent with conversation history | Multi-turn conversation context preservation                | Agent maintains context across multiple conversation turns                   | Test follow-up questions reference earlier conversation context correctly                                 |
| Agent with conversation history | Long conversation handling and summarization                | Long conversations are managed without losing important context              | Test 50+ turn conversation still has relevant context                                                     |
| Agent with conversation History | Conversation persistence across sessions                    | Conversations can be resumed after agent restart or timeout                  | Test conversation continues correctly after loading from DynamoDB                                         |
| Agent with knowledge base      | RAG-enhanced response generation | Agent retrieves relevant context and incorporates it into responses | Test question about company policy retrieves correct documents and cites them |
| Agent with knowledge base | Knowledge base query optimization | Agent formulates effective search queries from user input | Test conversational query gets reformulated into effective vector search |
| Agent with knowledge base | Source attribution and citation | Agent properly attributes information to source documents | Test response includes "According to document X..." or similar attribution |
| Agent with LLM                  | Streaming response handling                                 | Agent correctly processes and forwards streaming LLM responses               | Test converse\_stream() yields coherent response fragments                                                |
| Agent with LLM                  | LLM reasoning integration (for supported models)            | Reasoning-capable models provide thinking process correctly                  | Test Claude-3.7-Sonnet response includes <thinking> tags                                                  |
| Agent with tools                | LLM-initiated tool calling flow                             | LLM correctly decides to use tools and formats tool calls properly           | Test "What's the weather in Tokyo?" triggers weather tool with correct parameters                         |
| Agent with tools                | Multi-tool orchestration in single query                    | Agent can coordinate multiple tools for complex requests                     | Test "Plan a trip to Rome - weather, flights, and hotels" uses multiple tools                             |
| Agent with tools                | Tool error recovery and fallback handling                   | Agent gracefully handles tool failures and provides meaningful responses     | Test tool timeout or error still results in helpful agent response                                        |
| Agent with tools                | Tool parameter extraction accuracy                          | LLM correctly extracts and formats parameters from natural language          | Test "weather in San Francisco in Celsius" extracts city="San Francisco" and unit="Celsius"               |
| Agent with tracing              | Full trace captured for complex interactions                | All agent actions (LLM calls, tool uses, memory access) are properly traced  | Verify complex interaction generates complete trace tree (Generative AI toolkit infrastructure functions) |
| Agent with tracing              | Trace correlation across distributed components             | Traces maintain proper parent-child relationships in multi-component systems | Test trace IDs properly link related operations across services                                           |
| Multi-agent coordination        | Agent delegation and coordination                           | Supervisor agents effectively delegate to and coordinate specialist agents   | Test travel agent coordinates with weather, booking, and recommendation agents                            |
| Multi-agent communication       | Inter-agent communication and data flow                     | Agents can pass information and context between each other effectively       | Test output from one agent becomes proper input for another                                               |
| Multi-agent communication       | Agent isolation and independence                            | Each agent maintains its own conversation state and traces                   | Verify agents don't interfere with each other's memory or state  |
| Performance and scalability     |   Memory and resource usage optimization | Agent doesn't leak memory or resources during extended operation | Test agent handles 1000+ conversations without degraded performance |
| Performance and scalability       | Concurrent conversation handling         | Agent handles multiple simultaneous conversations correctly | Test 10+ concurrent users don't interfere with each other                    |
| Security and authentication     | Auth context enforcement across components                  | User authentication and authorization work correctly                         | Test users can only access their own conversations/ data                                                  |


## Acceptance tests


| Test focus | Test Description  | Rationale    | Example         |
|-----------------------------------------|-----------------------------------------------|--------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------|
| Business logic                 | Domain-specific business rules enforcement    | Agent correctly applies business logic and constraints                   | Test booking agent respects availability, pricing rules, and business policies                                                 |
| Business logic                 | Workflow and process automation               | Agent can guide users through complex multi-step processes               | Test agent walks user through loan application with proper validation at each step                                             |
| End-to-end user experience     | Complete user journey from question to answer | Validates the entire system works as users expect                        | Test: "Help me plan a weekend in Barcelona" returns comprehensive travel plan with weather, activities, restaurants            |
| End-to-end user experience     | Complex multi-domain query handling           | Agent can handle requests spanning multiple knowledge areas              | Test: "I need to prepare for a job interview at a tech company in XXX next month" (career advice + weather + company research) |
| Monitoring and observability   | Comprehensive logging and monitoring          | All important system events are properly logged and monitored            | Test critical events (errors, performance issues, security incidents) are captured and alertable                               |
| Monitoring and observability   | Debugging and troubleshooting capability      | Issues can be diagnosed and resolved using available telemetry           | Test ability to trace and diagnose specific conversation issues using logs and traces                                          |
|Performance requirements | Response latency under load | System meets performance SLAs under realistic usage patterns | Test 95% of queries complete within 5 seconds under normal load |
|Performance requirements | System availability and reliability | Agent maintains high availability during normal operation | Test system maintains >99.9% uptime over extended period | 
|Performance requirements | Graceful degradation under stress | System fails gracefully when overwhelmed rather than crashing | Test system behavior when receiving 10x normal traffic |
| Response quality               | Response accuracy and relevance               | Agent provides correct, helpful, and contextually appropriate responses  | Test domain-specific questions return accurate, up-to-date information                                                         |
| Response quality               | Response completeness and actionability       | Responses are complete and provide actionable information                | Test travel questions include specific recommendations with details (addresses, hours, prices)                                 |
| Response quality               | Appropriate response length and conciseness   | Responses are appropriately detailed without being verbose               | Test simple questions get concise answers, complex questions get thorough explanations                                         |
| Security and content filtering | Guardrails and content policy enforcement     | Content filtering works correctly across entire agent pipeline           | Test inappropriate content is blocked at input, during processing, and at output                                               |
| Security and safety            | Protect sensitive information                 | Agent doesn't expose or leak sensitive user information                  | Test agent doesn't include personal details from one user's conversation in another's                                          |
| Security and safety            | Malicious input handling                      | Agent safely handles attempts at prompt injection or system manipulation | Test various prompt injection techniques are properly mitigated (end-2-end)                                                    |
| Security and safety            | Content policy compliance                     | Agent outputs comply with organizational content policies                | Test agent refuses to generate harmful, biased, or inappropriate content                                                       |
